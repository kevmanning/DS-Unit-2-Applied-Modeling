{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DS_233_assignment_Kevin_Manning_dspt10.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevmanning/DS-Unit-2-Applied-Modeling/blob/master/DS_233_assignment_Kevin_Manning_dspt10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 3*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Permutation & Boosting\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your work.\n",
        "\n",
        "- [ ] If you haven't completed assignment #1, please do so first.\n",
        "- [ ] Continue to clean and explore your data. Make exploratory visualizations.\n",
        "- [ ] Fit a model. Does it beat your baseline? \n",
        "- [ ] Try xgboost.\n",
        "- [ ] Get your model's permutation importances.\n",
        "\n",
        "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
        "\n",
        "But, if you aren't ready to try xgboost and permutation importances with your dataset today, that's okay. You can practice with another dataset instead. You may choose any dataset you've worked with previously.\n",
        "\n",
        "The data subdirectory includes the Titanic dataset for classification and the NYC apartments dataset for regression. You may want to choose one of these datasets, because example solutions will be available for each.\n",
        "\n",
        "\n",
        "## Reading\n",
        "\n",
        "Top recommendations in _**bold italic:**_\n",
        "\n",
        "#### Permutation Importances\n",
        "- _**[Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)**_\n",
        "- [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
        "\n",
        "#### (Default) Feature Importances\n",
        "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
        "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
        "\n",
        "#### Gradient Boosting\n",
        "  - [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
        "  - [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), Chapter 8\n",
        "  - _**[Gradient Boosting Explained](https://www.gormanalysis.com/blog/gradient-boosting-explained/)**_ — Ben Gorman\n",
        "  - [Gradient Boosting Explained](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) — Alex Rogozhnikov\n",
        "  - [How to explain gradient boosting](https://explained.ai/gradient-boosting/) — Terence Parr & Jeremy Howard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoHGWczbLgs"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVUJvSiCbbmI"
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzpQAZPQbpSv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_profiling\n",
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKcBq33pbpNx"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded= files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GennIB1GbpJa"
      },
      "source": [
        "import io\n",
        "df= pd.read_csv(io.BytesIO(uploaded['nhl_17_18_reg_adv.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3QfUI5ZbpFI"
      },
      "source": [
        "df.columns =['game', 'date', 'home_away', 'opponent', 'goals', 'goals_against',\n",
        "       'win_loss', 'overtime', 'blank1', 'shots', 'penalty_mins', 'power_play_goals',\n",
        "       'power_plays', 'short_handed', 'blank2', 'opp_shots', 'opp_penalty_mins',\n",
        "       'opp_power_play_g', 'opp_power_plays', 'opp_short_handed', 'blank3', 'corsi_for',\n",
        "       'corsi_against', 'corsi_for_%', 'fenwick_for', 'fenwick_against',\n",
        "       'fenwick_%', 'face_off_win', 'face_off_loss', 'face_off_%',\n",
        "       'off_zone_start', 'pdo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VshyfD9EbpBR"
      },
      "source": [
        "# drop blank columns\n",
        "df= df.drop(['blank1', 'blank2', 'blank3'], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKgpyAjbo8E"
      },
      "source": [
        "# check majority class\n",
        "df['win_loss'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0bhGve4bo5e"
      },
      "source": [
        "# visually\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style= 'darkgrid')\n",
        "ax = sns.countplot(x=\"win_loss\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbTpdYnkbo1w"
      },
      "source": [
        "# distribution of target\n",
        "\n",
        "y= df['win_loss']\n",
        "print(y.nunique())\n",
        "print()\n",
        "print(y.value_counts())\n",
        "print()\n",
        "y.value_counts(normalize= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJzPSi2Tbox2"
      },
      "source": [
        "# drop unnecessary columns\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWsmELB3boti"
      },
      "source": [
        "df= df.drop(columns= ['date', 'game'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1rK3xETbopU"
      },
      "source": [
        "# begin to clean up the data\n",
        "df.isna().sum().sort_values(ascending= False)\n",
        "\n",
        "# looks like overtime and home/away are the only columns with issues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRq9P0IDbolT"
      },
      "source": [
        "# fill home/away first\n",
        "# make home game = 2 (fill NaN)\n",
        "# make away game = 1 (replace '@' with 1)\n",
        "df.home_away.value_counts(dropna= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdHZF-yEbohX"
      },
      "source": [
        "df.home_away= df.home_away.replace('@', 'away')\n",
        "df.home_away= df.home_away.fillna('home')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bq3qTbsboeB"
      },
      "source": [
        "df['home_away'].replace('@', 1)\n",
        "df.home_away.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPNe-LviboZt"
      },
      "source": [
        "# now do overtime\n",
        "df.overtime.value_counts(dropna= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KW5A4OcQh-"
      },
      "source": [
        "df.overtime= df.overtime.fillna('No')\n",
        "df.overtime= df.overtime.replace('SO', 'Yes')\n",
        "df.overtime= df.overtime.replace('OT', 'Yes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ90uCshcQcX"
      },
      "source": [
        "df.overtime.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBAZ38sAcQXB"
      },
      "source": [
        "# check to see if data cleaning worked\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcYnOjx4cQR2"
      },
      "source": [
        "target= 'win_loss'\n",
        "features= df.columns.drop([target])\n",
        "X= df[features]\n",
        "y= df[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wk3Qi5scQMq"
      },
      "source": [
        "# train/val/test split\n",
        "# will split it train/test and then split val/test\n",
        "\n",
        "X_train, X_test_val, y_train, y_test_val= train_test_split(X, y, test_size= .39, random_state= 99)\n",
        "X_val, X_test, y_val, y_test= train_test_split(X_test_val, y_test_val, test_size= .5, random_state= 99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKfgIbAdcQJf"
      },
      "source": [
        "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXkaIrs_cQD_"
      },
      "source": [
        "pipeline= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    DecisionTreeClassifier(max_depth=7)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy: ', pipeline.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8WkbdBXcP_D"
      },
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "tree = pipeline.named_steps['decisiontreeclassifier']\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    tree, \n",
        "    out_file=None, \n",
        "    feature_names=X_train.columns, \n",
        "    class_names=y_train.unique().astype(str), \n",
        "    filled=True, \n",
        "    impurity=False,\n",
        "    proportion=True\n",
        ")\n",
        "\n",
        "graphviz.Source(dot_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M7Ir9cRcP5t"
      },
      "source": [
        "# decision tree\n",
        "dt = pipeline.named_steps['decisiontreeclassifier']\n",
        "importances = pd.Series(dt.feature_importances_, X_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBDAiv7cP0c"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "plt.figure(figsize=(10,n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL1M7QZ4cPwn"
      },
      "source": [
        "pipeline2 = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline2.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline2.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY2oi4q5cyGB"
      },
      "source": [
        "# random forest\n",
        "rf = pipeline2.named_steps['randomforestclassifier']\n",
        "importances2 = pd.Series(rf.feature_importances_, X_train.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "plt.figure(figsize=(10,n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances2.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYT0a5h5cx_W"
      },
      "source": [
        "# decision tree ROC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_pred_proba = pipeline.predict_proba(X_val)[:,-1] # probability for the last class \n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86bwgXgMcx4u"
      },
      "source": [
        "# decision tree ROC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_pred_proba = pipeline.predict_proba(X_val)[:,-1] # probability for the last class \n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oP82aZvcxyC"
      },
      "source": [
        "y_pred= pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLwk06G4cxrL"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRg3TJpjcxkh"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5oqdfqZcxeQ"
      },
      "source": [
        "y_pred == y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyCLWC3WcxYI"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuaMqRcNcxR-"
      },
      "source": [
        "X_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ybKmAf0cxNz"
      },
      "source": [
        "y_pred2= pipeline2.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU1Q99_IcxH8"
      },
      "source": [
        "column= 'pdo'\n",
        "\n",
        "pipeline3= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy= 'median'),\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "pipeline3.fit(X_train.drop(columns=column), y_train)\n",
        "score_without = pipeline3.score(X_val.drop(columns=column), y_val)\n",
        "print(f'Validation Accuracy without {column}: {score_without}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQVnGmIBcxBq"
      },
      "source": [
        "feature= 'pdo'\n",
        "X_val_permuted = X_val.copy()\n",
        "X_val_permuted[feature]= np.random.permutation(X_val[feature])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgGtvAKAdgTP"
      },
      "source": [
        "score_permuted= pipeline2.score(X_val_permuted, y_val)\n",
        "print(f'Validation Accuracy with {feature} permuted: {score_permuted}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65PkYk5dgMM"
      },
      "source": [
        "transformers= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy= 'median')\n",
        ")\n",
        "\n",
        "X_train_transformed= transformers.fit_transform(X_train)\n",
        "X_val_transformed= transformers.transform(X_val)\n",
        "\n",
        "model4= RandomForestClassifier(n_estimators=50, random_state=99, n_jobs=-1)\n",
        "model4.fit(X_train_transformed, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AvTuPHjdgEl"
      },
      "source": [
        "transformers= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy= 'median')\n",
        ")\n",
        "\n",
        "X_train_transformed= transformers.fit_transform(X_train)\n",
        "X_val_transformed= transformers.transform(X_val)\n",
        "\n",
        "model4= RandomForestClassifier(n_estimators=50, random_state=99, n_jobs=-1)\n",
        "model4.fit(X_train_transformed, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVCe19KZdf9c"
      },
      "source": [
        "features_names= X_val.columns.to_list()\n",
        "pd.Series(permuter.feature_importances_, features_names).sort_values(ascending= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvT6t31KeHtL"
      },
      "source": [
        "permuter= PermutationImportance(\n",
        "    model4,\n",
        "    scoring= 'accuracy',\n",
        "    n_iter= 5,\n",
        "    random_state=99\n",
        ")\n",
        "\n",
        "permuter.fit(X_val_transformed, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utZTiJcFd_PB"
      },
      "source": [
        "eli5.show_weights(\n",
        "    permuter,\n",
        "    top= None,\n",
        "    feature_names= features_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf5z1fGxdf48"
      },
      "source": [
        "# remove features with zero importance\n",
        "print('Shape before removing feature ', X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtaxX0Omdfw9"
      },
      "source": [
        "min_imp= 0\n",
        "mask= permuter.feature_importances_ > min_imp\n",
        "features= X_train.columns[mask]\n",
        "X_train= X_train[features]\n",
        "print('Shape AFTER removing feature ', X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRCUEz7GeLpf"
      },
      "source": [
        "X_val= X_val[features]\n",
        "\n",
        "pipeline5= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy= 'median'),\n",
        "    RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "pipeline5.fit(X_train, y_train)\n",
        "print('Validation accuracy', pipeline5.score(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXHh9-JeLgY"
      },
      "source": [
        "pipeline6= make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    XGBClassifier(n_estimators= 100, random_state= 99, n_jobs= -1)\n",
        ")\n",
        "\n",
        "pipeline6.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0sQCCr8eLYA"
      },
      "source": [
        "y_pred3= pipeline6.predict(X_val)\n",
        "print('Validation Accuracy: ', accuracy_score(y_val, y_pred3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzsbO-OeLSB"
      },
      "source": [
        "# encoder= ce.OrdinalEncoder()\n",
        "# X_train_encoded= encoder.fit_transform(X_train)\n",
        "# X_val_encoded= encoder.transform(X_val)\n",
        "\n",
        "\n",
        "# model= XGBClassifier(\n",
        "#     n_estimators= 1000,\n",
        "#     max_depth=11,\n",
        "#     learning_rate= 0.5,\n",
        "#     n_jobs= -1\n",
        "# )\n",
        "\n",
        "# eval_set= [(X_test, y_test)]\n",
        "\n",
        "# model.fit(X_train_encoded, y_train,\n",
        "#           eval_set= eval_set,\n",
        "#           eval_metric= 'merror',\n",
        "#           early_stopping_rounds=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G8Zg6pfeLJl"
      },
      "source": [
        "# I get an error every time I run the above XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yp51eKUdfsC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}